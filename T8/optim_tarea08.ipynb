{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curso de Optimización (DEMAT)\n",
    "## Tarea 8\n",
    "\n",
    "| Descripción:                         | Fechas               |\n",
    "|--------------------------------------|----------------------|\n",
    "| Fecha de publicación del documento:  | **Abril 11, 2022**   |\n",
    "| Fecha límite de entrega de la tarea: | **Mayo   1, 2022**   |\n",
    "\n",
    "\n",
    "### Indicaciones\n",
    "\n",
    "- Envie el notebook que contenga los códigos y las pruebas realizadas de cada ejercicio.\n",
    "- Si se requiren algunos scripts adicionales para poder reproducir las pruebas,\n",
    "  agreguelos en un ZIP junto con el notebook.\n",
    "- Genere un PDF del notebook y envielo por separado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejercicio 1 (3 puntos)\n",
    "\n",
    "Sea $x=(x_1, x_2, ..., x_n)$ la variable independiente.\n",
    "\n",
    "Programar las siguientes funciones y sus gradientes:\n",
    "\n",
    "- Función cuadrática \n",
    "\n",
    "$$ f(\\mathbf{x}) = 0.5\\mathbf{x}^\\top \\mathbf{A}\\mathbf{x} - \\mathbf{b}^\\top\\mathbf{x}. $$\n",
    "\n",
    "Si $\\mathbf{I}$ es la matriz identidad y $\\mathbf{1}$ es la matriz llena de 1's,\n",
    "ambas de tamaño $n$, entonces\n",
    "\n",
    "$$ \\mathbf{A} = n\\mathbf{I} + \\mathbf{1} = \n",
    "\\left[\\begin{array}{llll} n      & 0      & \\cdots & 0 \\\\\n",
    "                       0      & n      & \\cdots & 0 \\\\ \n",
    "                       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                       0      & 0      & \\cdots & n \\end{array}\\right]\n",
    "+ \\left[\\begin{array}{llll} 1    & 1      & \\cdots & 1 \\\\\n",
    "                       1      & 1      & \\cdots & 1 \\\\ \n",
    "                       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "                       1      & 1      & \\cdots & 1 \\end{array}\\right],  \\qquad\n",
    "\\mathbf{b} = \\left[\\begin{array}{l} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] $$\n",
    "\n",
    "\n",
    "- Función generalizada de Rosenbrock\n",
    "\n",
    "$$  f(x) = \\sum_{i=1}^{n-1} 100(x_{i+1} - x_i^2)^2 + (1 - x_{i} )^2  $$\n",
    "\n",
    "$$ x_0 = (-1.2, 1, -1.2, 1, ..., -1.2, 1) $$\n",
    "\n",
    "\n",
    "En la implementación de cada función y de su gradiente, se recibe como argumento la variable $x$\n",
    "y definimos $n$ como la longitud del arreglo $x$, y con esos datos aplicamos la \n",
    "definición correspondiente.\n",
    "\n",
    "Estas funciones van a ser usadas para probar los algoritmos de optimización.\n",
    "El  punto $x_0$ que aparece en la definición de cada función es el punto inicial\n",
    "que se sugiere para el algoritmo de optimización.\n",
    "\n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de la función cuadrática y su gradiente\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de la función tridiagonal generalizada y su gradiente\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Implementación de la función generalizada de Rosenbrock y su gradiente\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2 (3.5 puntos)\n",
    "\n",
    "Programar el método de gradiente conjugado no lineal de Fletcher-Reeves:\n",
    "\n",
    "---\n",
    "\n",
    "La implementación recibe como argumentos a la función objetivo $f$, su gradiente $\\nabla f$,\n",
    "un punto inicial $x_0$, el máximo número de iteraciones $N$ y una tolerancia $\\tau>0$.\n",
    "\n",
    "1. Calcular  $\\nabla f_0 = \\nabla f(x_0)$, $p_0 = -\\nabla f_0$ y hacer $res=0$.\n",
    "2. Para $k=0,1,..., N$:\n",
    "\n",
    "- Si $\\|\\nabla f_k\\|< \\tau$, hacer $res=1$ y terminar el ciclo\n",
    "- Usando backtracking calcular el tamaño de paso  $\\alpha_k$\n",
    "- Calcular $x_{k+1} = x_k + \\alpha_k p_k$\n",
    "- Calcular $\\nabla f_{k+1} = \\nabla f(x_{k+1})$\n",
    "- Calcular \n",
    "\n",
    "$$ \\beta_{k+1} = \\frac{\\nabla f_{k+1}^\\top \\nabla f_{k+1}}{\\nabla f_{k}^\\top\\nabla f_{k}}  $$ \n",
    "\n",
    "- Calcular \n",
    "\n",
    "$$ p_{k+1} = -\\nabla f_{k+1} + \\beta_{k+1} p_k $$\n",
    "\n",
    "3. Devolver $x_k, \\nabla f_k, k, res$\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "1. Escriba la función que implemente el algoritmo anterior.\n",
    "2. Pruebe el algoritmo usando para cada una de las funciones del \n",
    "   Ejercicio 1, tomando el punto $x_0$ que se indica.\n",
    "3. Fije $N=50000$, $\\tau = \\epsilon_m^{1/3}$.\n",
    "4. Para cada función del Ejercicio 1 cree el punto $x_0$ correspondiente\n",
    "   usado $n=2, 10, 20$ y ejecute el algoritmo.\n",
    "   Imprima\n",
    "   \n",
    "- n,\n",
    "- f(x0),\n",
    "- las primeras y últimas 4 entradas del punto $x_k$ que devuelve el algoritmo,\n",
    "- f(xk),\n",
    "- la norma del vector $\\nabla f_k$, \n",
    "- el  número $k$ de iteraciones realizadas,\n",
    "- la variable $res$ para saber si el algoritmo puedo converger.\n",
    "  \n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En esta celda puede poner el código de las funciones\n",
    "# o poner la instrucción para importarlas de un archivo .py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruebas realizadas \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 3 (3.5 puntos)\n",
    "\n",
    "Programar el método de gradiente conjugado no lineal de usando la fórmula de\n",
    "Hestenes-Stiefel:\n",
    "\n",
    "En este caso el algoritmo es igual al del Ejercicio 2, con excepción del cálculo de $\\beta_{k+1}$. Primero se calcula el vector $\\mathbf{y}_k$ y luego $\\beta_{k+1}$:\n",
    "\n",
    "$$ \\mathbf{y}_k =  \\nabla f_{k+1}-\\nabla f_{k} $$\n",
    "$$ \\beta_{k+1} =   \\frac{\\nabla f_{k+1}^\\top\\mathbf{y}_k }{\\nabla p_{k}^\\top\\mathbf{y}_k}  $$\n",
    "\n",
    "1. Repita el Ejercicio 2 usando la fórmula de Hestenes-Stiefel.\n",
    "2. ¿Cuál de los métodos es mejor para encontrar los óptimos de las funciones de prueba?\n",
    "\n",
    "### Solución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
