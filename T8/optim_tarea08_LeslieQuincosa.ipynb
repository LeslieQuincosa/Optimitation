{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lNNXDqVEF94"
      },
      "source": [
        "# Curso de Optimización (DEMAT)\n",
        "## Tarea 8\n",
        "###Leslie Janeth Quincosa Ramírez\n",
        "\n",
        "| Descripción:                         | Fechas               |\n",
        "|--------------------------------------|----------------------|\n",
        "| Fecha de publicación del documento:  | **Abril 11, 2022**   |\n",
        "| Fecha límite de entrega de la tarea: | **Mayo   1, 2022**   |\n",
        "\n",
        "\n",
        "### Indicaciones\n",
        "\n",
        "- Envie el notebook que contenga los códigos y las pruebas realizadas de cada ejercicio.\n",
        "- Si se requiren algunos scripts adicionales para poder reproducir las pruebas,\n",
        "  agreguelos en un ZIP junto con el notebook.\n",
        "- Genere un PDF del notebook y envielo por separado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ASBwG7hEF-B"
      },
      "source": [
        "---\n",
        "\n",
        "## Ejercicio 1 (3 puntos)\n",
        "\n",
        "Sea $x=(x_1, x_2, ..., x_n)$ la variable independiente.\n",
        "\n",
        "Programar las siguientes funciones y sus gradientes:\n",
        "\n",
        "- Función cuadrática \n",
        "\n",
        "$$ f(\\mathbf{x}) = 0.5\\mathbf{x}^\\top \\mathbf{A}\\mathbf{x} - \\mathbf{b}^\\top\\mathbf{x}. $$\n",
        "\n",
        "Si $\\mathbf{I}$ es la matriz identidad y $\\mathbf{1}$ es la matriz llena de 1's,\n",
        "ambas de tamaño $n$, entonces\n",
        "\n",
        "$$ \\mathbf{A} = n\\mathbf{I} + \\mathbf{1} = \n",
        "\\left[\\begin{array}{llll} n      & 0      & \\cdots & 0 \\\\\n",
        "                       0      & n      & \\cdots & 0 \\\\ \n",
        "                       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "                       0      & 0      & \\cdots & n \\end{array}\\right]\n",
        "+ \\left[\\begin{array}{llll} 1    & 1      & \\cdots & 1 \\\\\n",
        "                       1      & 1      & \\cdots & 1 \\\\ \n",
        "                       \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "                       1      & 1      & \\cdots & 1 \\end{array}\\right],  \\qquad\n",
        "\\mathbf{b} = \\left[\\begin{array}{l} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{array}\\right] $$\n",
        "\n",
        "\n",
        "- Función generalizada de Rosenbrock\n",
        "\n",
        "$$  f(x) = \\sum_{i=1}^{n-1} 100(x_{i+1} - x_i^2)^2 + (1 - x_{i} )^2  $$\n",
        "\n",
        "$$ x_0 = (-1.2, 1, -1.2, 1, ..., -1.2, 1) $$\n",
        "\n",
        "\n",
        "En la implementación de cada función y de su gradiente, se recibe como argumento la variable $x$\n",
        "y definimos $n$ como la longitud del arreglo $x$, y con esos datos aplicamos la \n",
        "definición correspondiente.\n",
        "\n",
        "Estas funciones van a ser usadas para probar los algoritmos de optimización.\n",
        "El  punto $x_0$ que aparece en la definición de cada función es el punto inicial\n",
        "que se sugiere para el algoritmo de optimización.\n",
        "\n",
        "\n",
        "### Solución:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yh1YOcXlEF-D"
      },
      "outputs": [],
      "source": [
        "# Implementación de la función cuadrática y su gradiente\n",
        "\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "from numpy.linalg import eigvals\n",
        "import sys\n",
        "\n",
        "def function(x):\n",
        "    n = x.size\n",
        "    xT = x.T\n",
        "    A = n*np.identity(n) + np.ones((n, n))\n",
        "    b = np.ones(n)\n",
        "    return 0.5*xT@A@x - b.T@x\n",
        "    \n",
        "def gradient(x):\n",
        "    n = x.size\n",
        "    xT = x.T\n",
        "    A = n*np.identity(n) + np.ones((n, n))\n",
        "    b = np.ones(n)\n",
        "    return xT@A - b.T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AcTCxJ33EF-F"
      },
      "outputs": [],
      "source": [
        "from numpy.core.function_base import geomspace\n",
        "#  Implementación de la función generalizada de Rosenbrock y su gradiente\n",
        "\n",
        "def Rosenbrock(x):\n",
        "    return np.sum( 100*(x[1:] - x[:-1]**2)**2 + (1-x[:-1])**2)\n",
        "\n",
        "def gradRosenbrock(x):\n",
        "    n = x.size\n",
        "    g = np.zeros(n)\n",
        "    g[0] = 200*(x[1]-x[0]**2)*(-2*x[0])-2*(1-x[0])\n",
        "    g[n-1] = 200*(x[n-1]-x[n-2]**2)\n",
        "    g[1:-1] = 200*(x[1:-1]-x[:-2]**2) -400*x[1:-1]*(x[2:]-x[1:-1]**2)-2*(1-x[1:-1])\n",
        "    return g\n",
        "\n",
        "def vectorx0(n):\n",
        "    x = np.zeros(n)\n",
        "    for k in range( int(n/2)):\n",
        "      x[2*k] = -1.2\n",
        "      x[2*k+1] = 1\n",
        "    return x\n",
        "\n",
        "#x[:-1] me da el vector sin el último\n",
        "#x[1:] comienza en la segunda posición  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsDO0ZBHEF-I"
      },
      "source": [
        "\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvs_beySEF-K"
      },
      "source": [
        "## Ejercicio 2 (3.5 puntos)\n",
        "\n",
        "Programar el método de gradiente conjugado no lineal de Fletcher-Reeves:\n",
        "\n",
        "---\n",
        "\n",
        "La implementación recibe como argumentos a la función objetivo $f$, su gradiente $\\nabla f$,\n",
        "un punto inicial $x_0$, el máximo número de iteraciones $N$ y una tolerancia $\\tau>0$.\n",
        "\n",
        "1. Calcular  $\\nabla f_0 = \\nabla f(x_0)$, $p_0 = -\\nabla f_0$ y hacer $res=0$.\n",
        "2. Para $k=0,1,..., N$:\n",
        "\n",
        "- Si $\\|\\nabla f_k\\|< \\tau$, hacer $res=1$ y terminar el ciclo\n",
        "- Usando backtracking calcular el tamaño de paso  $\\alpha_k$\n",
        "- Calcular $x_{k+1} = x_k + \\alpha_k p_k$\n",
        "- Calcular $\\nabla f_{k+1} = \\nabla f(x_{k+1})$\n",
        "- Calcular \n",
        "\n",
        "$$ \\beta_{k+1} = \\frac{\\nabla f_{k+1}^\\top \\nabla f_{k+1}}{\\nabla f_{k}^\\top\\nabla f_{k}}  $$ \n",
        "\n",
        "- Calcular \n",
        "\n",
        "$$ p_{k+1} = -\\nabla f_{k+1} + \\beta_{k+1} p_k $$\n",
        "\n",
        "3. Devolver $x_k, \\nabla f_k, k, res$\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "---\n",
        "\n",
        "1. Escriba la función que implemente el algoritmo anterior.\n",
        "2. Pruebe el algoritmo usando para cada una de las funciones del \n",
        "   Ejercicio 1, tomando el punto $x_0$ que se indica.\n",
        "3. Fije $N=50000$, $\\tau = \\epsilon_m^{1/3}$.\n",
        "4. Para cada función del Ejercicio 1 cree el punto $x_0$ correspondiente\n",
        "   usado $n=2, 10, 20$ y ejecute el algoritmo.\n",
        "   Imprima\n",
        "   \n",
        "- n,\n",
        "- f(x0),\n",
        "- las primeras y últimas 4 entradas del punto $x_k$ que devuelve el algoritmo,\n",
        "- f(xk),\n",
        "- la norma del vector $\\nabla f_k$, \n",
        "- el  número $k$ de iteraciones realizadas,\n",
        "- la variable $res$ para saber si el algoritmo puedo converger.\n",
        "  \n",
        "\n",
        "### Solución:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JbjpK4fCEF-L"
      },
      "outputs": [],
      "source": [
        "# En esta celda puede poner el código de las funciones\n",
        "# o poner la instrucción para importarlas de un archivo .py\n",
        "#Implementación de la función de Fletcher-Reeves\n",
        "def Backtraking(f, fk, gk, xk, pk, a0, rho, c):\n",
        "    a = a0\n",
        "    while f(xk + a*pk) > fk +c*a*gk.T@pk:\n",
        "        a = rho*a\n",
        "    return a\n",
        "\n",
        "def FletcherReeves(f, g, x0, N, tol):\n",
        "    res = 0\n",
        "    xk = x0\n",
        "    for k in range(N):\n",
        "      gk = g(xk)\n",
        "      norm = LA.norm(gk)\n",
        "      if norm < tol:\n",
        "          res = 1\n",
        "          break\n",
        "      else:\n",
        "          pk = -gk\n",
        "          fk = f(xk)\n",
        "          gxk1 = g(xk)   #gradiente gk\n",
        "          ak = Backtraking(f, fk, gk, xk, pk, 2, 0.6, 0.01)\n",
        "          xk = xk + ak*pk\n",
        "          gk = g(xk)        #gk+1\n",
        "          bk = (gk.T @ gk)/(gxk1.T @ gxk1)\n",
        "          pk = -gk + bk*pk\n",
        "          if k+1 >= N:\n",
        "              res = 0\n",
        "              break\n",
        "    return xk, fk, gk, k, res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FFvbH7P4EF-M"
      },
      "outputs": [],
      "source": [
        "# Pruebas realizadas \n",
        "\n",
        "def Probar(f, g, n, N, tol):\n",
        "    x0 = vectorx0(n)\n",
        "    xk, fk, gk, k, res = FletcherReeves(f, g, x0, N, tol)\n",
        "    if res == 1:\n",
        "        print('¡Convergió! :)')\n",
        "    else:\n",
        "        print('¡No convergió! :(')\n",
        "    print('Valor n:', n)\n",
        "    print('f(x0):', f(x0))\n",
        "    print('xk:', xk[:2], '...', xk[n-2:])\n",
        "    print('f(xk):', fk)\n",
        "    print('||fk||:', LA.norm(fk))\n",
        "    print('Valor k:', k)\n",
        "    print('res:', res)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pruebas \n",
        "eps = sys.float_info.epsilon\n",
        "tol = eps**(1/3)\n",
        "ns = [2, 10, 20]\n",
        "\n",
        "for n in ns:\n",
        "  Probar(Rosenbrock, gradRosenbrock, n, 50000, tol)\n",
        "  print(\"\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJHwJxS2jqAr",
        "outputId": "0b9662a5-8453-4a66-beb3-fd8cd497bb12"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Convergió! :)\n",
            "Valor n: 2\n",
            "f(x0): 24.199999999999996\n",
            "xk: [1.00000463 1.00000929] ... [1.00000463 1.00000929]\n",
            "f(xk): 2.1578189665312123e-11\n",
            "||fk||: 2.1578189665312123e-11\n",
            "Valor k: 15665\n",
            "res: 1\n",
            "\n",
            "¡Convergió! :)\n",
            "Valor n: 10\n",
            "f(x0): 2057.0\n",
            "xk: [0.99999999 0.99999997] ... [0.99999641 0.99999281]\n",
            "f(xk): 1.7185202555415e-11\n",
            "||fk||: 1.7185202555415e-11\n",
            "Valor k: 19374\n",
            "res: 1\n",
            "\n",
            "¡Convergió! :)\n",
            "Valor n: 20\n",
            "f(x0): 4598.0\n",
            "xk: [1. 1.] ... [0.99999649 0.99999296]\n",
            "f(xk): 1.6479489759246556e-11\n",
            "||fk||: 1.6479489759246556e-11\n",
            "Valor k: 21238\n",
            "res: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugT_pOeOEF-O"
      },
      "source": [
        "\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uL2fvFfnEF-O"
      },
      "source": [
        "## Ejercicio 3 (3.5 puntos)\n",
        "\n",
        "Programar el método de gradiente conjugado no lineal de usando la fórmula de\n",
        "Hestenes-Stiefel:\n",
        "\n",
        "En este caso el algoritmo es igual al del Ejercicio 2, con excepción del cálculo de $\\beta_{k+1}$. Primero se calcula el vector $\\mathbf{y}_k$ y luego $\\beta_{k+1}$:\n",
        "\n",
        "$$ \\mathbf{y}_k =  \\nabla f_{k+1}-\\nabla f_{k} $$\n",
        "$$ \\beta_{k+1} =   \\frac{\\nabla f_{k+1}^\\top\\mathbf{y}_k }{\\nabla p_{k}^\\top\\mathbf{y}_k}  $$\n",
        "\n",
        "1. Repita el Ejercicio 2 usando la fórmula de Hestenes-Stiefel.\n",
        "2. ¿Cuál de los métodos es mejor para encontrar los óptimos de las funciones de prueba?\n",
        "\n",
        "### Solución:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MA4UWtAjEF-P"
      },
      "outputs": [],
      "source": [
        "#Método HestenesStiefel\n",
        "\n",
        "def HestenesStiefel(f, g, x0, N, tol):\n",
        "    res = 0\n",
        "    xk = x0\n",
        "    for k in range(N):\n",
        "      gk = g(xk)\n",
        "      norm = LA.norm(gk)\n",
        "      if norm < tol:\n",
        "          res = 1\n",
        "          break\n",
        "      else:\n",
        "          pk = -gk\n",
        "          fk = f(xk)\n",
        "          gxk1 = g(xk)   #gradiente gk\n",
        "          ak = Backtraking(f, fk, gk, xk, pk, 2, 0.5, 0.01)\n",
        "          xk = xk + ak*pk\n",
        "          gk = g(xk)        #gk+1\n",
        "          yk = gk - gxk1\n",
        "          bk = (gk.T @ yk)/(pk.T @ yk)\n",
        "          pk = -gk + bk*pk\n",
        "          if k+1 >= N:\n",
        "              res = 0\n",
        "              break\n",
        "    return xk, fk, gk, k, res\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Probar2(f, g, n, N, tol):\n",
        "    x0 = vectorx0(n)\n",
        "    xk, fk, gk, k, res = HestenesStiefel(f, g, x0, N, tol)\n",
        "    if res == 1:\n",
        "        print('¡Convergió! :)')\n",
        "    else:\n",
        "        print('¡No convergió! :(')\n",
        "    print('Valor n:', n)\n",
        "    print('f(x0):', f(x0))\n",
        "    print('xk:', xk[:2], '...', xk[n-2:])\n",
        "    print('f(xk):', fk)\n",
        "    print('||fk||:', LA.norm(fk))\n",
        "    print('Valor k:', k)\n",
        "    print('res:', res)\n",
        "\n"
      ],
      "metadata": {
        "id": "YKA6NmEPLbCl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eps = sys.float_info.epsilon\n",
        "tol = eps**(1/3)\n",
        "\n",
        "for n in ns:\n",
        "  Probar2(Rosenbrock, gradRosenbrock, n, 50000, tol)\n",
        "  print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiKqDglqLlrM",
        "outputId": "a3c7f46f-a3d7-4339-a180-a423f15b5ace"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Convergió! :)\n",
            "Valor n: 2\n",
            "f(x0): 24.199999999999996\n",
            "xk: [0.99999528 0.99999055] ... [0.99999528 0.99999055]\n",
            "f(xk): 2.2349270630763973e-11\n",
            "||fk||: 2.2349270630763973e-11\n",
            "Valor k: 11341\n",
            "res: 1\n",
            "\n",
            "¡Convergió! :)\n",
            "Valor n: 10\n",
            "f(x0): 2057.0\n",
            "xk: [0.99999999 0.99999997] ... [0.99999605 0.99999209]\n",
            "f(xk): 2.0821293276906092e-11\n",
            "||fk||: 2.0821293276906092e-11\n",
            "Valor k: 18605\n",
            "res: 1\n",
            "\n",
            "¡Convergió! :)\n",
            "Valor n: 20\n",
            "f(x0): 4598.0\n",
            "xk: [1. 1.] ... [0.99999607 0.99999212]\n",
            "f(xk): 2.062874650131741e-11\n",
            "||fk||: 2.062874650131741e-11\n",
            "Valor k: 20488\n",
            "res: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**¿Cuál de los métodos es mejor para encontrar los óptimos de las funciones de prueba?**\n",
        "\n",
        "Ambos métodos convergen y son muy parecidos, pero es más eficiente ya que el número de iteraciones $k$ es menor.\n",
        " "
      ],
      "metadata": {
        "id": "PyvRVeyTPOOx"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "optim_tarea08.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}